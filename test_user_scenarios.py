#!/usr/bin/env python3
"""
ç”¨æˆ·åœºæ™¯éªŒè¯å’Œæ€§èƒ½å¯¹æ¯”è„šæœ¬
éªŒè¯æ•°æ®æ¸…æ´—æ™ºèƒ½ä½“åœ¨å®é™…ä½¿ç”¨åœºæ™¯ä¸­çš„æ•ˆæœï¼Œå¹¶ä¸ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œå¯¹æ¯”
"""

import sys
import os
import json
import asyncio
import time
from pathlib import Path
from typing import Dict, Any, List, Optional
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# çœŸå®ç”¨æˆ·æµ‹è¯•åœºæ™¯
USER_SCENARIOS = {
    "scenario_1_basic_analysis": {
        "name": "åŸºç¡€è´¢æŠ¥åˆ†æ",
        "description": "ç”¨æˆ·ä¸Šä¼ åŸºç¡€çš„ä¸­æ–‡è´¢æŠ¥æ•°æ®ï¼Œè¦æ±‚è¿›è¡Œè´¢åŠ¡åˆ†æ",
        "user_data": {
            "åˆ©æ¶¦è¡¨": {
                "è¥ä¸šæ”¶å…¥": 573.88,
                "å‡€åˆ©æ¶¦": 11.04,
                "è¥ä¸šæˆæœ¬": 552.84,
                "è¥ä¸šåˆ©æ¶¦": 11.04
            },
            "èµ„äº§è´Ÿå€ºè¡¨": {
                "æ€»èµ„äº§": 3472.98,
                "æ€»è´Ÿå€º": 3081.02,
                "æ‰€æœ‰è€…æƒç›Š": 391.96
            },
            "å†å²æ•°æ®": {
                "2025": {"è¥ä¸šæ”¶å…¥": 573.88, "å‡€åˆ©æ¶¦": 11.04},
                "2024": {"è¥ä¸šæ”¶å…¥": 1511.39, "å‡€åˆ©æ¶¦": 36.11},
                "2023": {"è¥ä¸šæ”¶å…¥": 1420.56, "å‡€åˆ©æ¶¦": 32.45}
            }
        },
        "expected_outcomes": [
            "æˆåŠŸè¯†åˆ«ä¸­æ–‡æ•°æ®æ ¼å¼",
            "å†å²æ•°æ®æ­£ç¡®è§£æ",
            "ç”Ÿæˆæ ‡å‡†åŒ–çš„è´¢åŠ¡æ¯”ç‡",
            "æä¾›è´¨é‡è¯„ä¼°æŠ¥å‘Š"
        ]
    },
    
    "scenario_2_mixed_format": {
        "name": "æ··åˆæ ¼å¼å¤„ç†",
        "description": "ç”¨æˆ·æä¾›ä¸­è‹±æ–‡æ··åˆã€æ ¼å¼ä¸ä¸€è‡´çš„å¤æ‚æ•°æ®",
        "user_data": {
            "income_statement": {
                "revenue": 2500.0,
                "net_profit": 300.0,
                "operating_profit": 350.0
            },
            "åˆ©æ¶¦è¡¨": {
                "è¥ä¸šæ”¶å…¥": 1500.0,
                "å‡€åˆ©æ¶¦": 180.0
            },
            "èµ„äº§è´Ÿå€ºè¡¨": {
                "æ€»èµ„äº§": 8000.0,
                "total_liabilities": 5000.0
            },
            "å†å²æ•°æ®": {
                "2024": {
                    "revenue": 2000.0,
                    "net_profit": 250.0,
                    "è¥ä¸šæ”¶å…¥": 1500.0
                },
                "2023": {
                    "è¥ä¸šæ”¶å…¥": 1800.0,
                    "net_profit": 220.0
                }
            }
        },
        "expected_outcomes": [
            "æ™ºèƒ½è¯†åˆ«å’Œæ˜ å°„ä¸­è‹±æ–‡å­—æ®µ",
            "åˆå¹¶é‡å¤æ•°æ®",
            "æ ‡å‡†åŒ–æ•°æ®ç»“æ„",
            "ä¿æŒæ•°æ®å®Œæ•´æ€§"
        ]
    },
    
    "scenario_3_problematic_data": {
        "name": "é—®é¢˜æ•°æ®ä¿®å¤",
        "description": "æ•°æ®å­˜åœ¨å„ç§é—®é¢˜ï¼Œéœ€è¦æ™ºèƒ½ä¿®å¤å’Œæ¸…æ´—",
        "user_data": {
            "åˆ©æ¶¦è¡¨": {
                "è¥ä¸šæ”¶å…¥": 0,  # æ”¶å…¥ä¸º0
                "å‡€åˆ©æ¶¦": -500.0,  # äºæŸ
                "è¥ä¸šæˆæœ¬": "invalid_value",  # æ— æ•ˆå€¼
                "è¥ä¸šåˆ©æ¶¦": None  # ç©ºå€¼
            },
            "èµ„äº§è´Ÿå€ºè¡¨": {
                "æ€»èµ„äº§": None  # ç¼ºå°‘å…³é”®å­—æ®µ
            },
            "å†å²æ•°æ®": {
                "2024": {
                    "è¥ä¸šæ”¶å…¥": 1000.0
                    # ç¼ºå°‘å‡€åˆ©æ¶¦
                },
                "2023": {
                    "è¥ä¸šæ”¶å…¥": 0,
                    "å‡€åˆ©æ¶¦": -200.0
                }
            }
        },
        "expected_outcomes": [
            "è¯†åˆ«å’Œå¤„ç†æ— æ•ˆæ•°æ®",
            "å¡«è¡¥ç¼ºå¤±å­—æ®µ",
            "å¤„ç†å¼‚å¸¸æ•°å€¼",
            "æä¾›ä¿®å¤å»ºè®®"
        ]
    },
    
    "scenario_4_large_dataset": {
        "name": "å¤§æ•°æ®é›†å¤„ç†",
        "description": "ç”¨æˆ·ä¸Šä¼ å¤šå¹´ä»½çš„è¯¦ç»†è´¢åŠ¡æ•°æ®",
        "user_data": {
            "åŸºæœ¬ä¿¡æ¯": {
                "å…¬å¸åç§°": "å¤§å‹åˆ¶é€ ä¼ä¸š",
                "è‚¡ç¥¨ä»£ç ": "600000.SH"
            },
            "åˆ©æ¶¦è¡¨": {
                "è¥ä¸šæ”¶å…¥": 50000.0,
                "è¥ä¸šæˆæœ¬": 40000.0,
                "å‡€åˆ©æ¶¦": 5000.0
            },
            "èµ„äº§è´Ÿå€ºè¡¨": {
                "æ€»èµ„äº§": 200000.0,
                "æ€»è´Ÿå€º": 120000.0,
                "æ‰€æœ‰è€…æƒç›Š": 80000.0
            },
            "ç°é‡‘æµé‡è¡¨": {
                "ç»è¥æ´»åŠ¨ç°é‡‘æµé‡å‡€é¢": 15000.0,
                "æŠ•èµ„æ´»åŠ¨ç°é‡‘æµé‡å‡€é¢": -8000.0,
                "ç­¹èµ„æ´»åŠ¨ç°é‡‘æµé‡å‡€é¢": -3000.0
            },
            "å†å²æ•°æ®": {
                **{str(2025-i): {
                    "è¥ä¸šæ”¶å…¥": 50000 + i * 5000,
                    "å‡€åˆ©æ¶¦": 5000 + i * 500,
                    "è¥ä¸šæˆæœ¬": 40000 + i * 4000,
                    "æ€»èµ„äº§": 200000 + i * 10000,
                    "æ€»è´Ÿå€º": 120000 + i * 6000
                } for i in range(15)  # 15å¹´æ•°æ®
            }
        },
        "expected_outcomes": [
            "é«˜æ•ˆå¤„ç†å¤§æ•°æ®é›†",
            "ä¿æŒè‰¯å¥½æ€§èƒ½",
            "ç”Ÿæˆå‡†ç¡®çš„è¶‹åŠ¿åˆ†æ",
            "æä¾›æœ‰ä»·å€¼çš„ä¸šåŠ¡æ´å¯Ÿ"
        ]
    }
}


class UserScenarioValidator:
    """ç”¨æˆ·åœºæ™¯éªŒè¯å™¨"""
    
    def __init__(self):
        self.validation_results = {
            'total_scenarios': 0,
            'successful_scenarios': 0,
            'failed_scenarios': 0,
            'performance_metrics': {},
            'scenario_details': []
        }
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–éªŒè¯ç»„ä»¶"""
        try:
            from utu.agents.data_cleanser_agent import DataCleanserAgent
            from utu.tools.data_cleansing_toolkit import DataCleansingToolkit
            from utu.tools.financial_analysis_toolkit import StandardFinancialAnalyzer
            from utu.config import ToolkitConfig
            
            self.cleanser_agent = DataCleanserAgent()
            
            toolkit_config = ToolkitConfig(config={}, name="data_cleansing")
            self.cleansing_toolkit = DataCleansingToolkit(toolkit_config)
            
            self.financial_analyzer = StandardFinancialAnalyzer()
            
            logger.info("éªŒè¯ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"éªŒè¯ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def run_all_validations(self):
        """è¿è¡Œæ‰€æœ‰ç”¨æˆ·åœºæ™¯éªŒè¯"""
        logger.info("å¼€å§‹è¿è¡Œç”¨æˆ·åœºæ™¯éªŒè¯")
        
        overall_start_time = time.time()
        
        # é€ä¸ªéªŒè¯åœºæ™¯
        for scenario_id, scenario_data in USER_SCENARIOS.items():
            logger.info(f"\n{'='*60}")
            logger.info(f"éªŒè¯åœºæ™¯: {scenario_data['name']}")
            logger.info(f"æè¿°: {scenario_data['description']}")
            logger.info(f"{'='*60}")
            
            self.validation_results['total_scenarios'] += 1
            
            try:
                # éªŒè¯å•ä¸ªåœºæ™¯
                result = self.validate_scenario(scenario_id, scenario_data)
                
                if result['success']:
                    self.validation_results['successful_scenarios'] += 1
                    logger.info(f"âœ… åœºæ™¯ '{scenario_data['name']}' éªŒè¯æˆåŠŸ")
                else:
                    self.validation_results['failed_scenarios'] += 1
                    logger.error(f"âŒ åœºæ™¯ '{scenario_data['name']}' éªŒè¯å¤±è´¥")
                
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                self.validation_results['performance_metrics'][scenario_id] = result['performance']
                
                # è®°å½•è¯¦ç»†ç»“æœ
                self.validation_results['scenario_details'].append(result)
                
            except Exception as e:
                self.validation_results['failed_scenarios'] += 1
                logger.error(f"âŒ åœºæ™¯ '{scenario_data['name']}' éªŒè¯å¼‚å¸¸: {str(e)}")
        
        overall_duration = time.time() - overall_start_time
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report(overall_duration)
        
        # è¾“å‡ºæ€»ç»“
        self.print_validation_summary()
        
        return self.validation_results['successful_scenarios'] == len(USER_SCENARIOS)
    
    def validate_scenario(self, scenario_id: str, scenario_data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å•ä¸ªç”¨æˆ·åœºæ™¯"""
        result = {
            'scenario_id': scenario_id,
            'scenario_name': scenario_data['name'],
            'success': False,
            'performance': {},
            'validation_results': {},
            'recommendations': []
        }
        
        start_time = time.time()
        
        try:
            # 1. æ•°æ®æ¸…æ´—éªŒè¯
            cleansing_result = self._validate_data_cleansing(
                scenario_data['user_data'], scenario_data['expected_outcomes']
            )
            result['validation_results']['data_cleansing'] = cleansing_result
            
            # 2. æ€§èƒ½éªŒè¯
            performance_result = self._validate_performance(
                scenario_data['user_data'], scenario_id
            )
            result['performance'] = performance_result
            
            # 3. ä¸šåŠ¡ä»·å€¼éªŒè¯
            business_result = self._validate_business_value(
                scenario_data['user_data'], scenario_data['expected_outcomes']
            )
            result['validation_results']['business_value'] = business_result
            
            # 4. ç»¼åˆè¯„ä¼°
            overall_success = (
                cleansing_result['success'] and
                performance_result['within_limits'] and
                business_result['meets_expectations']
            )
            
            result['success'] = overall_success
            result['duration'] = time.time() - start_time
            
            # ç”Ÿæˆå»ºè®®
            result['recommendations'] = self._generate_recommendations(
                cleansing_result, performance_result, business_result
            )
            
        except Exception as e:
            logger.error(f"åœºæ™¯éªŒè¯å¼‚å¸¸: {str(e)}")
            result['duration'] = time.time() - start_time
            result['error'] = str(e)
        
        return result
    
    def _validate_data_cleansing(self, user_data: Dict[str, Any], expected_outcomes: List[str]) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®æ¸…æ´—æ•ˆæœ"""
        result = {
            'success': False,
            'outcomes_met': [],
            'outcomes_missed': [],
            'quality_score': 0,
            'details': []
        }
        
        try:
            # ä½¿ç”¨æ•°æ®æ¸…æ´—å·¥å…·é›†å¤„ç†æ•°æ®
            start_time = time.time()
            cleansing_result = self.cleansing_toolkit.cleanse_financial_data(user_data)
            processing_time = time.time() - start_time
            
            if cleansing_result['success']:
                # è¯„ä¼°ç»“æœè´¨é‡
                quality_score = cleansing_result['quality_score']
                quality_level = cleansing_result['quality_level']
                
                result['quality_score'] = quality_score
                result['processing_time'] = processing_time
                
                # æ£€æŸ¥é¢„æœŸç»“æœ
                for outcome in expected_outcomes:
                    if self._check_outcome(cleansing_result, outcome):
                        result['outcomes_met'].append(outcome)
                        result['details'].append(f"âœ… {outcome}")
                    else:
                        result['outcomes_missed'].append(outcome)
                        result['details'].append(f"âŒ {outcome}")
                
                # æ•°æ®æ¸…æ´—æˆåŠŸçš„åŸºæœ¬è¦æ±‚
                has_cleansed_data = 'cleansed_data' in cleansing_result
                has_quality_info = 'quality_score' in cleansing_result
                
                result['success'] = (
                    has_cleansed_data and
                    has_quality_info and
                    quality_score >= 50  # æœ€ä½è´¨é‡è¦æ±‚
                )
                
                if result['success']:
                    result['details'].append(f"è´¨é‡ç­‰çº§: {quality_level}")
                    result['details'].append(f"å¤„ç†æ—¶é—´: {processing_time:.3f}s")
                
            else:
                result['details'].append(f"æ•°æ®æ¸…æ´—å¤±è´¥: {cleansing_result.get('error', 'Unknown error')}")
                
        except Exception as e:
            result['details'].append(f"æ•°æ®æ¸…æ´—éªŒè¯å¼‚å¸¸: {str(e)}")
        
        return result
    
    def _validate_performance(self, user_data: Dict[str, Any], scenario_id: str) -> Dict[str, Any]:
        """éªŒè¯æ€§èƒ½è¡¨ç°"""
        result = {
            'success': False,
            'within_limits': False,
            'performance_metrics': {},
            'benchmark_comparison': {}
        }
        
        try:
            # è®¾ç½®æ€§èƒ½åŸºå‡†
            benchmarks = {
                'max_processing_time': 10.0,  # æœ€å¤§å¤„ç†æ—¶é—´10ç§’
                'min_quality_score': 60,    # æœ€ä½è´¨é‡åˆ†æ•°60
                'max_memory_estimate': 50   # æœ€å¤§å†…å­˜ä¼°ç®—50MB
            }
            
            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            start_time = time.time()
            
            # æ‰§è¡Œå¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
            test_runs = 3
            processing_times = []
            quality_scores = []
            
            for i in range(test_runs):
                test_start = time.time()
                test_result = self.cleansing_toolkit.cleanse_financial_data(user_data)
                test_end = time.time()
                
                if test_result.get('success'):
                    processing_times.append(test_end - test_start)
                    quality_scores.append(test_result.get('quality_score', 0))
            
            if processing_times:
                avg_processing_time = sum(processing_times) / len(processing_times)
                avg_quality_score = sum(quality_scores) / len(quality_scores)
                
                result['performance_metrics'] = {
                    'avg_processing_time': avg_processing_time,
                    'avg_quality_score': avg_quality_score,
                    'test_runs': test_runs,
                    'processing_time_variance': max(processing_times) - min(processing_times)
                }
                
                # æ€§èƒ½è¯„ä¼°
                time_ok = avg_processing_time <= benchmarks['max_processing_time']
                quality_ok = avg_quality_score >= benchmarks['min_quality_score']
                
                result['within_limits'] = time_ok and quality_ok
                result['success'] = True
                
                # åŸºå‡†å¯¹æ¯”
                result['benchmark_comparison'] = {
                    'time_performance': f"{avg_processing_time:.3f}s (åŸºå‡†: {benchmarks['max_processing_time']:.1f}s)",
                    'quality_performance': f"{avg_quality_score:.1f} (åŸºå‡†: {benchmarks['min_quality_score']})"
                }
                
                if not time_ok:
                    result['benchmark_comparison']['time_status'] = "è¶…å‡ºåŸºå‡†"
                if not quality_ok:
                    result['benchmark_comparison']['quality_status'] = "ä½äºåŸºå‡†"
                
                result['details'] = [
                    f"å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.3f}s",
                    f"å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality_score:.1f}",
                    f"æµ‹è¯•è¿è¡Œæ¬¡æ•°: {test_runs}"
                ]
                
                if not result['within_limits']:
                    result['details'].append("æ€§èƒ½è¶…å‡ºåŸºå‡†é™åˆ¶")
                
        except Exception as e:
            result['details'].append(f"æ€§èƒ½éªŒè¯å¼‚å¸¸: {str(e)}")
        
        return result
    
    def _validate_business_value(self, user_data: Dict[str, Any], expected_outcomes: List[str]) -> Dict[str, Any]:
        """éªŒè¯ä¸šåŠ¡ä»·å€¼"""
        result = {
            'success': False,
            'meets_expectations': False,
            'business_benefits': [],
            'limitations': [],
            'roi_assessment': ''
        }
        
        try:
            # æ•°æ®æ¸…æ´—ä»·å€¼è¯„ä¼°
            start_time = time.time()
            cleansing_result = self.cleansing_toolkit.cleanse_financial_data_data)
            cleansing_end = time.time()
            
            if cleansing_result.get('success'):
                cleansing_data = cleansing_result['cleansed_data']
                quality_score = cleansing_result['quality_score']
                
                # è¯„ä¼°ä¸šåŠ¡ä»·å€¼
                benefits = []
                limitations = []
                
                # 1. æ•°æ®æ ‡å‡†åŒ–ä»·å€¼
                if self._has_standardized_structure(cleansing_data):
                    benefits.append("æ•°æ®æ ¼å¼æ ‡å‡†åŒ–ï¼Œä¾¿äºåç»­åˆ†æ")
                
                # 2. è´¨é‡ä¿è¯ä»·å€¼
                if quality_score >= 80:
                    benefits.append(f"é«˜è´¨é‡æ•°æ®({quality_score:.1f}åˆ†)ï¼Œåˆ†æç»“æœå¯é ")
                elif quality_score >= 60:
                    benefits.append(f"ä¸­ç­‰è´¨é‡æ•°æ®({quality_score:.1f}åˆ†)ï¼ŒåŸºæœ¬å¯ç”¨")
                else:
                    limitations.append(f"æ•°æ®è´¨é‡è¾ƒä½({quality_score:.1f}åˆ†)ï¼Œéœ€è¦æ”¹è¿›")
                
                # 3. é”™è¯¯å¤„ç†ä»·å€¼
                issues_count = cleansing_result.get('issues_found', 0)
                if issues_count > 0:
                    benefits.append(f"å‘ç°{issues_count}ä¸ªæ•°æ®é—®é¢˜ï¼Œå¸®åŠ©æé«˜æ•°æ®è´¨é‡")
                else:
                    benefits.append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€é¢å¤–ä¿®å¤")
                
                # 4. æ•ˆç‡ä»·å€¼
                processing_time = cleansing_end - start_time
                if processing_time < 2.0:
                    benefits.append(f"å¤„ç†å¿«é€Ÿ({processing_time:.2f}s)ï¼Œç”¨æˆ·å‹å¥½")
                elif processing_time < 5.0:
                    benefits.append(f"å¤„ç†æ•ˆç‡åˆç†({processing_time:.2f}s)")
                else:
                    limitations.append(f"å¤„ç†æ—¶é—´è¾ƒé•¿({processing_time:.2f}s)ï¼Œå¯ä¼˜åŒ–")
                
                result['business_benefits'] = benefits
                result['limitations'] = limitations
                
                # ç»¼åˆè¯„ä¼°
                meets_basic_requirements = (
                    len(benefits) >= 2 and
                    quality_score >= 60
                )
                
                result['meets_expectations'] = meets_basic_requirements
                result['success'] = True
                
                # ROIè¯„ä¼°
                if meets_basic_requirements:
                    result['roi_assessment'] = "æ•°æ®æ¸…æ´—æ˜¾è‘—æå‡äº†æ•°æ®è´¨é‡ï¼Œä¸ºåç»­åˆ†æåˆ›é€ ä»·å€¼"
                else:
                    result['roi_assessment'] = "æ•°æ®æ¸…æ´—æä¾›äº†ä¸€äº›ä»·å€¼ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´"
                
                result['details'] = [
                    f"ä¸šåŠ¡æ”¶ç›Šæ•°: {len(benefits)}",
                    f"é™åˆ¶å› ç´ æ•°: {len(limitations)}",
                    f"ROIè¯„ä¼°: {result['roi_assessment']}"
                ]
                
            else:
                result['details'].append("æ•°æ®æ¸…æ´—å¤±è´¥ï¼Œæ— æ³•åˆ›é€ ä¸šåŠ¡ä»·å€¼")
                
        except Exception as e:
            result['details'].append(f"ä¸šåŠ¡ä»·å€¼éªŒè¯å¼‚å¸¸: {str(e)}")
        
        return result
    
    def _check_outcome(self, cleansing_result: Dict[str, Any], expected_outcome: str) -> bool:
        """æ£€æŸ¥é¢„æœŸç»“æœæ˜¯å¦è¾¾åˆ°"""
        outcome_lower = expected_outcome.lower()
        
        # å¸¸è§é¢„æœŸç»“æœçš„æ£€æŸ¥
        outcome_checks = {
            "ä¸­æ–‡": lambda r: any("ä¸­æ–‡" in str(r).lower() for r in cleansing_result.get('details', [])),
            "å†å²": lambda r: "historical" in str(r).lower() or "å†å²" in str(r).lower(),
            "æ ‡å‡†åŒ–": lambda r: "æ ‡å‡†åŒ–" in str(r).lower() or "standard" in str(r).lower(),
            "æ¯”ç‡": lambda r: "æ¯”ç‡" in str(r).lower() or "ratio" in str(r).lower(),
            "è´¨é‡": lambda r: "è´¨é‡" in str(r).lower() or "quality" in str(r).lower(),
            "ä¿®å¤": lambda r: "ä¿®å¤" in str(r).lower() or "fix" in str(r).lower(),
            "å¤„ç†": lambda r: "å¤„ç†" in str(r).lower() or "process" in str(r).lower()
        }
        
        for key, check_func in outcome_checks.items():
            if key in outcome_lower and check_func(cleansing_result):
                return True
        
        return False
    
    def _has_standardized_structure(self, data: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ ‡å‡†åŒ–ç»“æ„"""
        standard_indicators = [
            'income_statement', 'èµ„äº§è´Ÿå€ºè¡¨', 'cash_flow',
            'profitability', 'solvency', 'efficiency'
        ]
        
        return any(indicator in str(data).lower() for indicator in standard_indicators)
    
    def _generate_recommendations(self, cleansing_result: Dict, performance_result: Dict, business_result: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæ•°æ®æ¸…æ´—ç»“æœçš„å»ºè®®
        if not cleansing_result['success']:
            recommendations.append("å»ºè®®æ£€æŸ¥æ•°æ®æ ¼å¼ï¼Œç¡®ä¿ç¬¦åˆåŸºæœ¬è¦æ±‚")
        elif cleansing_result['quality_score'] < 70:
            recommendations.append("å»ºè®®æä¾›æ›´å®Œæ•´å’Œå‡†ç¡®çš„æ•°æ®ä»¥è·å¾—æ›´å¥½çš„åˆ†æç»“æœ")
        
        # åŸºäºæ€§èƒ½ç»“æœçš„å»ºè®®
        if not performance_result['within_limits']:
            recommendations.append("å»ºè®®ä¼˜åŒ–æ•°æ®å¤„ç†æµç¨‹ä»¥æé«˜æ€§èƒ½")
        elif performance_result['performance_metrics'].get('avg_processing_time', 0) > 5.0:
            recommendations.append("è€ƒè™‘ä½¿ç”¨ç¼“å­˜æˆ–æ‰¹å¤„ç†æ¥æé«˜å¤„ç†æ•ˆç‡")
        
        # åŸºäºä¸šåŠ¡ä»·å€¼çš„å»ºè®®
        if not business_result['meets_expectations']:
            recommendations.append("å»ºè®®æä¾›æ›´è¯¦ç»†å’Œç»“æ„åŒ–çš„è´¢åŠ¡æ•°æ®")
        elif business_result['limitations']:
            recommendations.append("è§£å†³å‘ç°çš„æ•°æ®è´¨é‡é—®é¢˜ä»¥è·å¾—æ›´å¥½çš„ä¸šåŠ¡æ´å¯Ÿ")
        
        # é€šç”¨å»ºè®®
        if len(recommendations) == 0:
            recommendations.append("æ•°æ®æ¸…æ´—æ•ˆæœè‰¯å¥½ï¼Œå¯ä»¥ç»§ç»­è¿›è¡Œè´¢åŠ¡åˆ†æ")
        
        return recommendations
    
    def generate_comparison_report(self, total_duration: float):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        logger.info("\n" + "="*60)
        logger.info("ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        logger.info("="*60)
        
        # æ”¶é›†æ€§èƒ½æ•°æ®
        all_performance = [
            {
                'scenario_id': scenario_id,
                'scenario_name': details['scenario_name'],
                'performance': details['performance']
            }
            for scenario_id, details in self.validation_results['scenario_details']
            if 'performance' in details
        ]
        
        if all_performance:
            avg_processing_time = sum(
                p['performance'].get('performance_metrics', {}).get('avg_processing_time', 0)
                for p in all_performance
            ) / len(all_performance)
            
            avg_quality_score = sum(
                p['performance'].get('performance_metrics', {}).get('avg_quality_score', 0)
                for p in all_performance
            ) / len(all_performance)
            
            logger.info(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.3f}ç§’")
            logger.info(f"å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality_score:.2f}åˆ†")
            
            # æ€§èƒ½åˆ†çº§
            if avg_processing_time < 2.0:
                performance_grade = "ä¼˜ç§€"
            elif avg_processing_time < 5.0:
                performance_grade = "è‰¯å¥½"
            elif avg_processing_time < 10.0:
                performance_grade = "å¯æ¥å—"
            else:
                performance_grade = "éœ€æ”¹è¿›"
            
            logger.info(f"æ•´ä½“æ€§èƒ½ç­‰çº§: {performance_grade}")
            
            # è´¨é‡åˆ†çº§
            if avg_quality_score >= 85:
                quality_grade = "ä¼˜ç§€"
            elif avg_quality_score >= 70:
                quality_grade = "è‰¯å¥½"
            elif avg_quality_score >= 60:
                quality_grade = "å¯æ¥å—"
            else:
                quality_grade = "éœ€æ”¹è¿›"
            
            logger.info(f"æ•´ä½“è´¨é‡ç­‰çº§: {quality_grade}")
    
    def print_validation_summary(self):
        """æ‰“å°éªŒè¯æ‘˜è¦"""
        logger.info("\n" + "="*60)
        logger.info("ç”¨æˆ·åœºæ™¯éªŒè¯æ‘˜è¦")
        logger.info("="*60)
        
        total = self.validation_results['total_scenarios']
        successful = self.validation_results['successful_scenarios']
        failed = self.validation_results['failed_scenarios']
        success_rate = (successful / total * 100) if total > 0 else 0
        
        logger.info(f"æ€»åœºæ™¯æ•°: {total}")
        logger.info(f"éªŒè¯æˆåŠŸ: {successful}")
        logger.info(f"éªŒè¯å¤±è´¥: {failed}")
        logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        # åˆ†ç±»ç»Ÿè®¡
        categories = {}
        for detail in self.validation_results['scenario_details']:
            if 'validation_results' in detail:
                for validation_type, validation_data in detail['validation_results'].items():
                    if validation_type not in categories:
                        categories[validation_type] = {'total': 0, 'success': 0}
                    categories[validation_type]['total'] += 1
                    if validation_data.get('success', False):
                        categories[validation_type]['success'] += 1
        
        logger.info("\néªŒè¯ç±»åˆ«ç»Ÿè®¡:")
        for category, stats in categories.items():
            rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
            logger.info(f"  {category}: {stats['success']}/{stats['total']} ({rate:.1f}%)")
        
        # è¯¦ç»†ç»“æœ
        logger.info("\nè¯¦ç»†éªŒè¯ç»“æœ:")
        for detail in self.validation_results['scenario_details']:
            status = "âœ… æˆåŠŸ" if detail['success'] else "âŒ å¤±è´¥"
            logger.info(f"  {status}: {detail['scenario_name']} ({detail['duration']:.3f}s)")
            
            if 'recommendations' in detail and detail['recommendations']:
                for rec in detail['recommendations'][:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªå»ºè®®
                    logger.info(f"    å»ºè®®: {rec}")
        
        logger.info("\n" + "="*60)
        logger.info("éªŒè¯å®Œæˆ")
        logger.info("="*60)
        
        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        self.save_validation_report()
    
    def save_validation_report(self):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        try:
            report = {
                "user_scenario_validation_summary": {
                    "total_scenarios": self.validation_results['total_scenarios'],
                    "successful_scenarios": self.validation_results['successful_scenarios'],
                    "failed_scenarios": self.validation_results['failed_scenarios'],
                    "success_rate": (self.validation_results['successful_scenarios'] / 
                                  self.validation_results['total_scenarios'] * 100) 
                                  if self.validation_results['total_scenarios'] > 0 else 0),
                    "validation_time": datetime.now().isoformat(),
                    "user_scenarios": list(USER_SCENARIOS.keys())
                },
                "performance_metrics": self.validation_results['performance_metrics'],
                "scenario_details": self.validation_results['scenario_details'],
                "system_readiness": "production_ready" if self.validation_results['successful_scenarios'] == len(USER_SCENARIOS) else "needs_improvement"
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            report_file = project_root / "user_scenario_validation_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ç”¨æˆ·åœºæ™¯éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜éªŒè¯æŠ¥å‘Šå¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    print("æ•°æ®æ¸…æ´—ç”¨æˆ·åœºæ™¯éªŒè¯å’Œæ€§èƒ½å¯¹æ¯”")
    print("=" * 50)
    
    try:
        # åˆ›å»ºéªŒè¯å™¨
        validator = UserScenarioValidator()
        
        # è¿è¡ŒéªŒè¯
        success = validator.run_all_validations()
        
        if success:
            print("\nğŸ‰ æ‰€æœ‰ç”¨æˆ·åœºæ™¯éªŒè¯é€šè¿‡ï¼")
            print("æ•°æ®æ¸…æ´—æ™ºèƒ½ä½“å·²å‡†å¤‡å¥½æœåŠ¡çœŸå®ç”¨æˆ·ã€‚")
            print("\nç³»ç»Ÿä¼˜åŠ¿:")
            print("âœ“ èƒ½å¤Ÿå¤„ç†å„ç§æ ¼å¼çš„è´¢åŠ¡æ•°æ®")
            print("âœ“ æ™ºèƒ½è¯†åˆ«å’Œä¿®å¤æ•°æ®é—®é¢˜")
            print("âœ“ æä¾›æ•°æ®è´¨é‡ä¿è¯")
            print("âœ“ æ€§èƒ½è¡¨ç°ä¼˜ç§€")
            print("âœ“ ä¸šåŠ¡ä»·å€¼æ˜¾è‘—")
            return True
        else:
            print(f"\nâš ï¸ æœ‰ {validator.validation_results['failed_scenarios']} ä¸ªåœºæ™¯éªŒè¯å¤±è´¥ã€‚")
            print("è¯·æ£€æŸ¥å¹¶æ”¹è¿›ç³»ç»Ÿåå†è¯•ã€‚")
            return False
            
    except Exception as e:
        print(f"\nâŒ ç”¨æˆ·åœºæ™¯éªŒè¯å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)